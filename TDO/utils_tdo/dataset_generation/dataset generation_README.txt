Benchmark Generation
The creation of the synthetic dataset requires (i) a set of data items for which (ii) a ground truth specifying the expected true values is known, (iii) a set of sources, and (iv) claimed claims for each of the sources. The description of the entire process for dataset generation phase is now presented for both the ontologies we used: DBpedia and GeneOntology. Based on DBpedia, we generated two datasets centered on dbpedia-owl:birthPlace and dbpedia-owl:genre predicates. As a similar procedure has been employed to generate the latter, we only detail here the description of the entire process for the dataset generation corresponding to dbpedia-owl:birthPlace predicate. In this case, we collected our ground truth by extracting a set of triples associated to the predicate dbpedia-owl:birthPlace from DBpedia. This set of claims D x V defines the set of data items D as well as the true values v*_d associated to each data item. 796186 data items have been extracted from DBpedia using this procedure. Among them, 534723 data items with a unique value exist. Indeed, cases in which the same subject has more than one object for the dbpedia-owl:birthPlace predicate have been excluded since in some situations these values are even conflicting. For instance the subject dbpedia.org/resource/Aurora_Quezon is associated with 5 different values. Among them, the values dbpedia.org/resource/Aurora_(province) and dbpedia.org/resource/Quezon are conflicting. Indeed, both are provinces of Philippines and it is well known that a person cannot be born in two difference provinces. Note that we assume that all claims in D x V are true, indeed, due to the way the dataset is generated, it will not impact our evaluation; considering these claims was mainly motivated by the will to mimic as much as possible a real scenario w.r.t. value distribution.
The partial ordering of values we consider has been constructed as follows. Using DBpedia 2015-04, we have selected all the triples that contain rdfs:subClassOf as predicate. Among them, we have extracted only the subset of classes that are subsumed by dbpedia-dbo:Place class. Then, we have loaded the triples involving the rdf:type predicate, and we have retained those that are instances of dbpedia-dbo:Place. Successively, all triples that have dbpedia-dbo:country predicate have been added, as well as the triple with dbpedia-dbo:isPartof predicate. At this point, the partial order has been populated using all triples selected so far without any distinction among the type of relationships considered. Since, dbpedia-owl:Thing is the most abstract concept in DBpedia, we have rooted all the concepts belonging to the graph with it. Then, in order to be sure to obtain a partial order of values, we have checked if the obtained RDF graph respects the properties of a DAG. Unfortunately, cycles induced by incorrectness on part-of triples had to be removed. To solve this problem, an heuristic has been applied to delete the cycles. For each of them, we have rejected the edge that have as target the node with the highest out-degree. Indeed, the heuristic we have employed hypothesizes that more abstract concepts should have higher out-degree that the less abstract ones. For instance, to remove the cycle existing between the resources dbpedia.org/resource/The_Bronx and dbpedia.org/resource/New_York_City, we compared their out-degree that are, respectively, equal to 6 and 65. According to our heuristic the concept New Y ork City is more abstract than The Bronx. Therefore, among the two edges, we eliminated the one whose target was dbpedia.org/resource/The_Bronx. The validity of this choice is confirmed by the reality. Indeed, the resource dbpedia.org/resource/The_Bronx represents one of the five boroughs of New York City, and therefore The Bronx is a part of New Y ork City. Analyzing the discarded edges, the behavior supposed by the heuristic has been well respected. After the elimination of the cycles, the partial ordering of values has been obtained. Then, to align the ground truth composed of triples associated to the predicate dbpedia-owl:birthPlace with the obtained partial ordering of values, only the triples that have the object presents in the partial order structure have been retained. As final step, a randomly selected subset of 10000 data items from D has been selected to generate our synthetic datasets. In order to consolidate the results we obtained, similarly to this procedure, we generated synthetic datasets using a different predicate, i.e. dbpedia-owl:genre, and its related partial ordering. 
In order to evaluate the behaviour of our approach and its interdependence to a particular ontology, tests have been performed with different partial ordering structures. Using Gene Ontology (GO) we generated three datasets related to the three facets of all gene product properties such as they are classiffed in GO, i.e. Cellular Component (CC), Molecular Function (MF) and Biological Process (BP). These three aspects are treated as three different predicates for the dataset generation procedure. In this case, the triples contained in the ground truth are expressed as <gene id+context id, predicate, function>. We introduced the notion of context id to tackle one drawback of the dataset. Indeed a lot of genes have more than one functions for the same aspects and we deal with functional predicates. In this situation, a gene and all its associated functions are treated as distinct cases. Each of the associated functions is considered as the truth in a specific case, i.e. context. This strategy does not impact the results of our experiments.We are not interested in the semantic meaning of the values, but in the structure among them. Therefore, all the values could be potentially replaced with others. For creating the ground truth for each predicate, we selected a subset of statements reported in the GO annotation file where all descriptions about the function of specific genes are reported. The extraction of the partial ordering of values related to these predicates was simpler than the one for deriving the partial ordering from the DBpedia. Indeed the GO is a direct acyclic graph composed of only is-a (if x is-a y, then x is a sub-type of y) and part-of (if y part-of x, then y implies x) relationships. Since all of them are ordering relationships, the construction of the partial order over the values was immediate. We selected all concepts subsumed by the concept CC, MF and BP without any additional operations. The entire procedure involving the selection of dbpedia-owl:birthPlace and dbpedia-owl:genre triples with the construction of the related partial ordering of values has been implemented using the Semantic Measures Library. Considering the ground truth and the partial ordering of values the rationale of the experiment setup has been to generate claims for fictive sources (each of them associated to fixed accuracies) and to evaluate methods by evaluating how well they estimate source accuracies. To this end, we have generated a set of sources with the related source trustworthiness level. In order to simulate as much as possible a real scenario, the general principle adopted in this study has been to assign to the majority of the sources a medium source trustworthiness score and to the minority of them a very low or high source trustworthiness level. To reproduce this behaviour, for generating source trustworthiness, we employ a Gaussian distribution with average and a standard deviation equals to, respectively, 0.6 and 0.4. The set of claims claimed by sources has been generated according to the following general rules: (1) A source does not provide a claim for each data item in the ground truth {we consider that sources only express values for some data items. We therefore consider that each source has a different coverage over the entire data item set. In particular, the following behavior is modeled: a lot of sources provide claims on few data items (in other words, on a few topics), while only few sources claim on a wide range of data items. This respects the real world situation where many websites are specialized on specific topics, while only few websites provide information on a broader range of themes. This rationale represents the long-tail phenomena that is common in many real world applications. It was mention for the first time in the truth-discovery domain by Li et al("A confidence aware approach for truth discovery on long-tail data", 2014). Note that it is also considered that a source only provides a single value for a specific data item (Fs_d is a singleton).
(2) A source provides a true claim with respect to its trustworthiness level. True values have been selected among the inclusive ancestors of the value specified in the ground truth - the set of inclusive ancestors of a value v is defined by {x|v <= x}. This procedure is in accordance with the definition of the set of true values we have considered so far. Note that the strategy used to select the true value also considers the similarity measure between true values and the true value specified into the ground truth. To this end, Lin's measure using Sanchez et al. information content has been used { technical aspects related to semantic measures are briefly summarized below; several existing studies contain more details about them. The semantic similarity of two values u, v that are defined into the partial ordering is computed using simLin:V x V -> [0; 1] such that simLin(u; v) = 2 * IC(MICA(u; v)) / (IC(u) + IC(v)) with IC a function used to compute the Information Content of a value by analysing the topology of the partial ordering (refer to the work of Sanchez and Batet "Semantic similarity estimation in the biomedical domain: An ontology-based information-theoretic perspective", 2011, for the formula used in this study) and MICA(u; v) the Most Informative Common Ancestor of the two values u; v, i.e. the value that generalizes both u and v which has the higher IC score. Semantic similarity computations have been performed using the Semantic Measures Library. Three different kinds of datasets: EXP, LOW E, UNI have been generated. They differ from each other regarding the strategy used to select the true value of true claims while generating the entire claim set, see Figure 3. Given a data item d 2 D, its true value v x d in the ground truth is used to derive V x d the set of true values according to our definition. Each value in this set is also correlated with a similarity measure to v x d; this similarity measure is used to order the values of V x d . Given this ordered set of true values, in the case of EXP datasets, the sources tend to provide values that are similar to the one in the  ground truth. Therefore, only a limited number of sources claim general true values for a specific data itemi. For the generation of UNI datasets, the true values provided by sources are selected independently of their similarity with the values in the ground truth. In the case of LOW E datasets, the majority of true values are selected like in UNI datasets (i.e. all the values have the same probability to be chosen), but in few cases there is a slightly higher probability to select values more similar than others to the truth. 
(3) A source provides a false claim with respect to its untrustworthy level (1 - trustworthiness level) and chooses the claim among the values in the partial ordering that are not ancestors nor descendants of the value specified in the ground truth. Considering v*_d to be the ground truth, the set of false values for a data item d is therefore defined by {V_d \ V*_d}\{x|x < v*_d}. Note that, also in this case, a false value is selected considering the similarity measure among values { the same aforementioned measure configuration has been used. Moreover, to better represent real use cases we also considered that false values tend to be repeated. Therefore, in the dataset generation procedure, a false value already provided has a higher probability to be selected than others (those that have never been observed so far). In this case only a single exponential law governs the picking of false values. Considering the similarity measure between the true value v*_d and the set of possible false values, it means that there is a higher probability to select false values that are more similar to the truth than values that are more different, i.e. less similar. This happens also in real world scenario. This setting also permits to reproduce malicious sources that use false value copy to spread false information. The false value domain is composed of all the values in the taxonomy that are not ancestors nor descendants of v*_d.Recall that the values subsuming or equal to the true value constitute the set of true values. The descendants represent potentially true values on which no other knowledge is given, they cannot therefore be considered all false a priori. For this reason, we removed them from false value domain.) 